{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weighted-orange",
   "metadata": {},
   "source": [
    "# Homework 3: film dialogue\n",
    "\n",
    "In our lab for Feb 15 we looked at two different ways to compare collections of texts: by examining specific words that are overrepresented in one collection relative to another, or by assessing the strength of a model that attempts to describe the boundary between the two.\n",
    "\n",
    "We'll pursue both of those strategies a little further in this homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-married",
   "metadata": {},
   "source": [
    "## Familiar preparation\n",
    "\n",
    "To start with we'll import useful modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "young-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-accreditation",
   "metadata": {},
   "source": [
    "#### read in the dialogue dataset\n",
    "\n",
    "It has one line for each character; the field ```lines``` contains all dialogue attributed to that character in the [Cornell Movie Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). Separate lines are delimited by slashes, but we'll ignore that here.\n",
    "\n",
    "We call the dataset ```chars``` because the it has one line per character.\n",
    "\n",
    "Notice that the path below has changed because the ```homeworks/``` directory is only one level down from the parent is417 directory. ```labs/Feb15Dialog/``` was two levels down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "owned-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogpath = Path('../data/movie_dialogue.tsv')\n",
    "\n",
    "chars = pd.read_csv(dialogpath, sep = '\\t')\n",
    "\n",
    "# let's also randomize the row order\n",
    "chars = chars.sample(frac = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "suspected-reporter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>cid</th>\n",
       "      <th>cname</th>\n",
       "      <th>mname</th>\n",
       "      <th>gender</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>year</th>\n",
       "      <th>genres</th>\n",
       "      <th>comedy</th>\n",
       "      <th>thriller</th>\n",
       "      <th>drama</th>\n",
       "      <th>romance</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>m334</td>\n",
       "      <td>u5054</td>\n",
       "      <td>KATHARINE</td>\n",
       "      <td>the english patient</td>\n",
       "      <td>f</td>\n",
       "      <td>1159</td>\n",
       "      <td>1996</td>\n",
       "      <td>['romance', 'drama', 'war']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Our Garden, our garden - not so much the garde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>m153</td>\n",
       "      <td>u2401</td>\n",
       "      <td>PETE</td>\n",
       "      <td>o brother, where art thou?</td>\n",
       "      <td>m</td>\n",
       "      <td>460</td>\n",
       "      <td>2000</td>\n",
       "      <td>['comedy', 'adventure', 'crime', 'music']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Good Lord, what do we do? / Awful sorry I betr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>m272</td>\n",
       "      <td>u4081</td>\n",
       "      <td>BLADE</td>\n",
       "      <td>blade</td>\n",
       "      <td>m</td>\n",
       "      <td>696</td>\n",
       "      <td>1998</td>\n",
       "      <td>['action', 'adventure', 'fantasy', 'horror', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Whistler, I -- / No, we can treat the wounds -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>m101</td>\n",
       "      <td>u1502</td>\n",
       "      <td>MIKE WALLACE</td>\n",
       "      <td>the insider</td>\n",
       "      <td>m</td>\n",
       "      <td>534</td>\n",
       "      <td>1999</td>\n",
       "      <td>['biography', 'drama', 'thriller']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Mike?\" / How grave? / No, no, we're fine... /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>m455</td>\n",
       "      <td>u6837</td>\n",
       "      <td>WILL</td>\n",
       "      <td>nothing but a man</td>\n",
       "      <td>m</td>\n",
       "      <td>303</td>\n",
       "      <td>1964</td>\n",
       "      <td>['drama', 'romance']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Didn't I tell you to beat it - huh? / Couldn't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mid    cid         cname                       mname gender  wordcount  \\\n",
       "1310  m334  u5054     KATHARINE         the english patient      f       1159   \n",
       "317   m153  u2401          PETE  o brother, where art thou?      m        460   \n",
       "964   m272  u4081         BLADE                       blade      m        696   \n",
       "24    m101  u1502  MIKE WALLACE                 the insider      m        534   \n",
       "1958  m455  u6837          WILL           nothing but a man      m        303   \n",
       "\n",
       "      year                                             genres  comedy  \\\n",
       "1310  1996                        ['romance', 'drama', 'war']   False   \n",
       "317   2000          ['comedy', 'adventure', 'crime', 'music']    True   \n",
       "964   1998  ['action', 'adventure', 'fantasy', 'horror', '...   False   \n",
       "24    1999                 ['biography', 'drama', 'thriller']   False   \n",
       "1958  1964                               ['drama', 'romance']   False   \n",
       "\n",
       "      thriller  drama  romance  \\\n",
       "1310     False   True     True   \n",
       "317      False  False    False   \n",
       "964       True  False    False   \n",
       "24        True   True    False   \n",
       "1958     False   True     True   \n",
       "\n",
       "                                                  lines  \n",
       "1310  Our Garden, our garden - not so much the garde...  \n",
       "317   Good Lord, what do we do? / Awful sorry I betr...  \n",
       "964   Whistler, I -- / No, we can treat the wounds -...  \n",
       "24    \"Mike?\" / How grave? / No, no, we're fine... /...  \n",
       "1958  Didn't I tell you to beat it - huh? / Couldn't...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-harvard",
   "metadata": {},
   "source": [
    "## A better way to validate models\n",
    "\n",
    "In our lab, we spent a lot of energy selecting test sets and training sets. We learned that if you want to produce a truly general model of a category like \"comedy,\" it's important to make sure your algorithm can't \"cheat\" by just memorizing (say) the names of characters who appear in a limited set of comedies.\n",
    "\n",
    "Defining train and test sets so they contain non-overlapping groups of movies produced a more general model of comedy, and a more realistic estimate of accuracy.\n",
    "\n",
    "But each of us got a slightly different measure of model accuracy, because our measure of accuracy depended entirely on a small subset of the data (1/5th of the movies) that we randomly chose as the test set.\n",
    "\n",
    "To get a more stable measure of accuracy, it's better to *cross-validate* your model, by repeatedly holding out a different 1/5th (or 1/10th) of the data as the test set, and training on the remainder. If we hold out a different test set each time we can eventually test on all the data, without ever testing on data that was included in our training set.\n",
    "\n",
    "We could do this ourselves by writing a loop, but scikit-learn also comes with a ```cross_validate()``` function that does it for us automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "united-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-circus",
   "metadata": {},
   "source": [
    "First we need to create a matrix of ```wordcounts``` that we'll use to make predictions, and define a useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "distributed-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cid' in chars.columns:         \n",
    "    character_ids = chars['cid']\n",
    "    chars = chars.set_index('cid')   # If we haven't made this the index yet, let's do it.\n",
    "else:\n",
    "    character_ids = chars.index.tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 8000)\n",
    "sparse_counts = vectorizer.fit_transform(chars['lines']) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "wordcounts = pd.DataFrame(sparse_counts.toarray(), index = character_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "wordcounts.head()\n",
    "\n",
    "# We'll also define a useful function\n",
    "\n",
    "def gendertonumber(astring):\n",
    "    if astring.lower() == 'f':  # note that we lowercase before checking:\n",
    "        return 1                    # this version of the function is slightly\n",
    "    else:                           # better than the one in the lab notebook\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-vegetation",
   "metadata": {},
   "source": [
    "Having done that, we can proceed to train models.\n",
    "\n",
    "In the lines below we cross-validate a model of romance, while making sure that the data is grouped by movie-id. If you're curious about GroupKFold (or any other aspect of scikit-learn), you can always [inspect the documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html) Basically, what's happening here is that we define an object that splits the data up into five parts, and then instruct it to use movie-id to do the splitting.\n",
    "\n",
    "What happens if you change the number of splits (```n_splits```) to 10, or to 2? If you see slight changes in accuracy, reflect on why they're happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "express-public",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6850795759733364"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_y = chars['romance'].astype(int)\n",
    "bayes = MultinomialNB(alpha = 1)\n",
    "grouper = GroupKFold(n_splits = 5)\n",
    "cv_results = cross_validate(bayes, wordcounts, all_y, groups = chars['mid'], cv = grouper)\n",
    "np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-review",
   "metadata": {},
   "source": [
    "The ```cross_validate()``` function produces a different test_score for each of the five separate test sets. Above, we take the mean score, but we can also inspect them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "sensitive-blast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.56459904, 0.56746197, 0.55672193, 0.54945993, 0.5609231 ]),\n",
       " 'score_time': array([0.07308292, 0.07102227, 0.07092404, 0.07096982, 0.07038021]),\n",
       " 'test_score': array([0.70707071, 0.72390572, 0.6952862 , 0.71380471, 0.71838111])}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-toyota",
   "metadata": {},
   "source": [
    "Notice that if we take out the ```groups``` and ```cv``` parameters it no longer divides the test and training sets by movie-id. In that case, we get an unrealistically high accuracy for genre, because genre becomes easy to predict if you can memorize the vocabulary of specific movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "radical-empire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8231829253751682"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_y = chars['romance'].astype(int)\n",
    "bayes = MultinomialNB(alpha = 1)\n",
    "cv_results = cross_validate(bayes, wordcounts, all_y)\n",
    "np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-springer",
   "metadata": {},
   "source": [
    "We can also use this same function to model gender. We just have to change the way the response variable *y* is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "consecutive-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7322301145235378\n"
     ]
    }
   ],
   "source": [
    "all_y = chars['gender'].map(gendertonumber)\n",
    "bayes = MultinomialNB(alpha = 1)\n",
    "grouper = GroupKFold(n_splits = 5)\n",
    "cv_results = cross_validate(bayes, wordcounts, all_y, groups = chars['mid'], cv = grouper)\n",
    "score = np.mean(cv_results['test_score'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-baking",
   "metadata": {},
   "source": [
    "## Assignment 1. Changing the smoothing parameter of the model\n",
    "\n",
    "Write a little loop that tests the gender model above while varying the ```alpha``` parameter of the model.\n",
    "\n",
    "Remember, this is the Laplacian smoothing--a number that gets added to the count of each word to acknowledge that it probably *sometimes* occurs in dialogue spoken by women (or by men), even if not in this sample.\n",
    "\n",
    "**1a.** Try each of the settings in this list: [0.001, 1, 2, 4, 8, 16, 32, 64]. In each case print ```alpha,``` and the resulting accuracy. \n",
    "\n",
    "**1b.** Then do the same thing--in a separate cell--for a model of genre (say, romance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-accident",
   "metadata": {},
   "source": [
    "**1c.** Write a reflection on these results. Why might alphas higher than one improve the model? Why is the ideal alpha not the same for every category? (Your answer may be a little speculative; that's okay; speculate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-cleaners",
   "metadata": {},
   "source": [
    "## Assignment 2: Words overrepresented in romance.\n",
    "\n",
    "In the lab on Feb 15 we practiced ways to identify words that are overrepresented in one corpus relative to another.\n",
    "\n",
    "If you review [Ben Schmidt's blog post about comparing corpuses,](http://sappingattention.blogspot.com/2011/10/comparing-corpuses-by-word-use.html), you'll realize that the methods we used in that lab were fairly crude. We were just asking how much larger a word's frequency is in one corpus than in the other, or how many *times* larger its frequency is. As Schmidt's post shows, there may be better ways to make that comparison, and in future weeks we'll explore them.\n",
    "\n",
    "But for right now, let's just practice the simple ratio test. **2a** Create a list of 25 words that are many times more common in romances than in other movies, and **2b** a list of 25 words that are many times more common in other movies than in romances. Compare these lists to the similar test we performed on gender categories in the lab. **2c** Interpret the evidence. The evidence you get from this comparison will not be decisive, but what hypotheses come to mind as deserving further investigation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-character",
   "metadata": {},
   "source": [
    "## Assignment 3: Do gender stereotypes weaken over time?\n",
    "\n",
    "It probably doesn't make sense to try to answer this question with a single big model of all movies, because the gender norms involved in shaping dialogue could have *changed* over time.\n",
    "\n",
    "Instead, let's find the median release date for characters, and divide our dataset into two roughly evenly-sized groups (early and late). Then we can cross-validate models to predict gender in each half of the dataset, and see if the accuracy of the later model is lower.\n",
    "\n",
    "In the lab notebook, I proposed running this test many times, in order to measure uncertainty, and draw inferences about significance. But the technique of \"sampling with replacement\" may require more explanation. I'll demonstrate a way to do that in the homework solution, but for right now just cross-validate a single model for each half of the timeline -- using the methods from assignment 1 -- and see what result you get.\n",
    "\n",
    "You may want to try different values of \"alpha,\" as we did in Assignment 1, and use the value that performs best for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a accuracy of a gender model based on movies released 1995 or before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "necessary-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b accuracy of a gender model based on movies released after 1995"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
