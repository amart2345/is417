{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2: Identifying distinctive words\n",
    "\n",
    "We're going to explore some basic forms of text analysis, using David Robinson's dataset of tweets made from the account of Donald J. Trump.\n",
    "\n",
    "To begin, let's import some modules we're going to need later, and also read in the Trump data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, math, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of identifying \"distinctive\" words\n",
    "\n",
    "In this section we'll explore Dunning's log-likelihood, and also think about the strengths and weaknesses of \"distinctive\" words as evidence.\n",
    "\n",
    "First let's glance at the Trump dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  1512 rows, and\n",
      "18 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>False</td>\n",
       "      <td>9214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 15:20:44</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762669882571980801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>3107</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>False</td>\n",
       "      <td>6981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 13:28:20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762641595439190016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>2390</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>False</td>\n",
       "      <td>15724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 00:05:54</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762439658911338496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6691</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>False</td>\n",
       "      <td>19837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 23:09:08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762425371874557952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6402</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>False</td>\n",
       "      <td>34051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 21:31:46</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762400869858115588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>11717</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             1   \n",
       "1           1             2   \n",
       "2           2             3   \n",
       "3           3             4   \n",
       "4           4             5   \n",
       "\n",
       "                                                text  favorited  \\\n",
       "0  My economic policy speech will be carried live...      False   \n",
       "1  Join me in Fayetteville, North Carolina tomorr...      False   \n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...      False   \n",
       "3  Michael Morell, the lightweight former Acting ...      False   \n",
       "4  The media is going crazy. They totally distort...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0           9214       NaN  2016-08-08 15:20:44      False         NaN   \n",
       "1           6981       NaN  2016-08-08 13:28:20      False         NaN   \n",
       "2          15724       NaN  2016-08-08 00:05:54      False         NaN   \n",
       "3          19837       NaN  2016-08-07 23:09:08      False         NaN   \n",
       "4          34051       NaN  2016-08-07 21:31:46      False         NaN   \n",
       "\n",
       "                   id  replyToUID  \\\n",
       "0  762669882571980801         NaN   \n",
       "1  762641595439190016         NaN   \n",
       "2  762439658911338496         NaN   \n",
       "3  762425371874557952         NaN   \n",
       "4  762400869858115588         NaN   \n",
       "\n",
       "                                        statusSource       screenName  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  realDonaldTrump   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...  realDonaldTrump   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...  realDonaldTrump   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...  realDonaldTrump   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...  realDonaldTrump   \n",
       "\n",
       "   retweetCount  isRetweet  retweeted  longitude  latitude  \n",
       "0          3107      False      False        NaN       NaN  \n",
       "1          2390      False      False        NaN       NaN  \n",
       "2          6691      False      False        NaN       NaN  \n",
       "3          6402      False      False        NaN       NaN  \n",
       "4         11717      False      False        NaN       NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relativepath = Path('../../data/trump.csv')\n",
    "trump = pd.read_csv(relativepath, encoding = 'latin1')\n",
    "print('We have ', trump.shape[0], 'rows, and')\n",
    "print(trump.shape[1], 'columns.')\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic functions\n",
    "\n",
    "For a lot of the work we do today, we're going to want to construct dictionaries that hold the frequencies of words in different categories: poetry or fiction, Trump-iphone or Trump-android. To do this we'll need to break text into words, count the words in each text, and then add up the counts by category.\n",
    "\n",
    "Let's define some functions that do this. (You can find more polished versions of these functions in the ```nltk``` module.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 't', 'co', 'https', 'to', 'and', 'a', 'i', 'in', 'is']\n"
     ]
    }
   ],
   "source": [
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "def create_vocab(seq_of_strings, n):\n",
    "    ''' Given a sequence of text snippets, this function\n",
    "    returns the n most common words. We'll use this to\n",
    "    create a limited 'vocabulary'.\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    \n",
    "    for astring in seq_of_strings:\n",
    "        thesewords = [x for x in split_into_words(astring) if len(x) > 0]\n",
    "        these_counts = Counter(thesewords)\n",
    "        vocab = vocab + these_counts\n",
    "        \n",
    "    topn = [x[0] for x in vocab.most_common(n)]\n",
    "    return topn\n",
    "\n",
    "# Let's test the vocabulary function.\n",
    "vocab = create_vocab(trump['text'], 4000)\n",
    "print(vocab[0:10])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few more basic functions\n",
    "\n",
    "Once we have a vocabulary, we're going to want to divide our texts into categories, create Counters summing the word frequencies in those categories, and then compare the two Counters to find words that are overrepresented in one category relative to the other.\n",
    "\n",
    "There are several ways we could define \"overrepresented.\" We'll use Robinson's simple log-odds measure, as well as Dunning's log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "two thousand 2000\n",
      "ten 10\n",
      "\n",
      "BOTTOM VALUES:\n",
      "neg one -1\n",
      "zero 0\n"
     ]
    }
   ],
   "source": [
    "def logodds(countsA, countsB, word):\n",
    "    ''' Straightforward.\n",
    "    '''\n",
    "    \n",
    "    odds = (countsA[word] + 1) / (countsB[word] + 1)\n",
    "    \n",
    "    # Why do we add 1 on both sides? Two reasons. The hacky one is \n",
    "    # that otherwise we'll get a division-by-zero error whenever\n",
    "    # word isn't present in countsB. The more principled reason\n",
    "    # is that this technique (called Laplacian smoothing) tends\n",
    "    # to reduce the dramatic disproportion likely to be found in\n",
    "    # very rare words.\n",
    "    \n",
    "    return math.log(odds)\n",
    "\n",
    "def signed_dunnings(countsA, totalA, countsB, totalB, word):\n",
    "    ''' Less straightforward. This function calculates a signed (+1 / -1)\n",
    "    version of Dunning's log likelihood. Intuitively, this is a number \n",
    "    that gets larger as the frequency of the word in our two corpora\n",
    "    diverges from its EXPECTED frequency -- i.e., the frequency it would\n",
    "    have if it were equally distributed over both. But it also tends to get\n",
    "    larger as the raw frequency of the word increases.\n",
    "    \n",
    "    Note that this function requires two additional arguments:\n",
    "    the total number of words in A and B. We could calculate that inside\n",
    "    the function, but it's faster to calculate it just once, outside the function.\n",
    "    \n",
    "    Also note: the strict definition of Dunnings has no 'sign': it gets bigger\n",
    "    whether a word is overrepresented in A or B. I've edited that so that Dunnings\n",
    "    is positive if overrepresented in A, and negative if overrepresented in B.\n",
    "    '''\n",
    "    if word not in countsA and word not in countsB:\n",
    "        return 0\n",
    "    \n",
    "    # the raw frequencies of this word in our two corpora\n",
    "    # still doing a little Laplacian smoothing here\n",
    "    a = countsA[word] + 0.1\n",
    "    b = countsB[word] + 0.1\n",
    "    \n",
    "    # now let's calculate the expected number of times this\n",
    "    # word would occur in both if the frequency were constant\n",
    "    # across both\n",
    "    overallfreq = (a + b) / (totalA + totalB)\n",
    "    expectedA = totalA * overallfreq\n",
    "    expectedB = totalB * overallfreq\n",
    "    \n",
    "    # and now the Dunning's formula\n",
    "    dunning = 2 * ((a * math.log(a / expectedA)) + (b * math.log(b / expectedB)))\n",
    "    \n",
    "    if a < expectedA:\n",
    "        return -dunning\n",
    "    else:   \n",
    "        return dunning\n",
    "\n",
    "# a set of common words is often useful\n",
    "stopwords = {'a', 'an', 'are', 'and', 'but', 'or', 'that', 'this', 'so', \n",
    "             'all', 'at', 'if', 'in', 'i', 'is', 'was', 'by', 'of', 'to', \n",
    "             'the', 'be', 'you', 'were'}\n",
    "\n",
    "# finally, one more function: given a list of tuples like\n",
    "testlist = [(10, 'ten'), (2000, 'two thousand'), (0, 'zero'), (-1, 'neg one'), (8, 'eight')]\n",
    "# we're going to want to sort them and print the top n and bottom n\n",
    "\n",
    "def headandtail(tuplelist, n):\n",
    "    tuplelist.sort(reverse = True)\n",
    "    print(\"TOP VALUES:\")\n",
    "    for i in range(n):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "    \n",
    "    print()\n",
    "    print(\"BOTTOM VALUES:\")\n",
    "    lastindex = len(tuplelist) - 1\n",
    "    for i in range(lastindex, lastindex - n, -1):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "        \n",
    "headandtail(testlist, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Is Dunning's a better measure than logodds for Trump's tweets?\n",
    "\n",
    "Let's put all these functions together to answer that question.\n",
    "\n",
    "I've sketched the outline of a program below in \"pseudocode,\" which\n",
    "describes what needs to be done. Translate that into real Python code, using\n",
    "the functions defined above. First use Robinson's logodds function and try to\n",
    "replicate his results. See what happens if you do (or don't) remove stopwords\n",
    "and tweets that begin with a quote.\n",
    "                                                   \n",
    "Then edit your code to use Dunning's log likelihood. Does that seem to be a better (more revealing) measure of overrepresentation? How would we decide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "imwithyou 3.044522437723423\n",
      "trumppence16 2.70805020110221\n",
      "join 2.662587827025453\n",
      "americafirst 2.6390573296152584\n",
      "votetrump 2.4849066497880004\n",
      "video 2.4849066497880004\n",
      "7pm 2.4849066497880004\n",
      "trump2016 2.439444275711243\n",
      "rncincle 2.3978952727983707\n",
      "inprimary 2.3978952727983707\n",
      "https 2.221013439300234\n",
      "co 2.2044841373490236\n",
      "wiprimary 2.1972245773362196\n",
      "officers 2.1972245773362196\n",
      "fitn 2.1972245773362196\n",
      "crookedhillary 2.1972245773362196\n",
      "3pm 2.0794415416798357\n",
      "tickets 2.0149030205422647\n",
      "votetrumpnh 1.9459101490553132\n",
      "refugees 1.9459101490553132\n",
      "gopconvention 1.9459101490553132\n",
      "tomorrow 1.791759469228055\n",
      "greatly 1.791759469228055\n",
      "gopdebate 1.791759469228055\n",
      "daytona 1.791759469228055\n",
      "\n",
      "BOTTOM VALUES:\n",
      "realdonaldtrump -4.143134726391533\n",
      "p -3.1780538303479458\n",
      "00 -2.9444389791664407\n",
      "v -2.890371757896165\n",
      "â¼ã -2.772588722239781\n",
      "two -2.70805020110221\n",
      "badly -2.639057329615259\n",
      "crazy -2.639057329615259\n",
      "megynkelly -2.639057329615259\n",
      "weak -2.5649493574615367\n",
      "re -2.4849066497880004\n",
      "spent -2.4849066497880004\n",
      "u -2.4849066497880004\n",
      "four -2.3978952727983707\n",
      "joke -2.3978952727983707\n",
      "strong -2.3978952727983707\n",
      "dead -2.3025850929940455\n",
      "such -2.3025850929940455\n",
      "talking -2.3025850929940455\n",
      "because -2.268683541318364\n",
      "ago -2.1972245773362196\n",
      "guns -2.1972245773362196\n",
      "h -2.1972245773362196\n",
      "mails -2.1972245773362196\n",
      "anyone -2.0794415416798357\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(trump['text'], 5000)\n",
    "vocab = list(set(vocab) - stopwords)\n",
    "\n",
    "numrows = trump.shape[0]\n",
    "android = Counter()\n",
    "iphone = Counter()\n",
    "\n",
    "for i in range(numrows):\n",
    "        \n",
    "    counts = Counter(split_into_words(trump['text'][i]))\n",
    "    if 'iphone' in trump['statusSource'][i]:\n",
    "        iphone = iphone + counts\n",
    "    elif 'android' in trump['statusSource'][i]:\n",
    "        android = android + counts\n",
    "\n",
    "iphonesum = sum(iphone.values())\n",
    "androidsum = sum(android.values())\n",
    "\n",
    "tuplelist = []\n",
    "\n",
    "for word in vocab:\n",
    "    # g = signed_dunnings(iphone, iphonesum, android, androidsum, word)\n",
    "    g = logodds(iphone, android, word)\n",
    "    tuplelist.append((g, word))\n",
    "\n",
    "headandtail(tuplelist, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Apply the same methods to a more literary dataset.\n",
    "\n",
    "I've also provided a dataset of roughly 1026 snippets from nineteenth-century poetry and fiction. The code below should read it in. Run that, then copy and paste the code you worked up for Trump, and edit it so it provides the most distinctive words for poetry (versus fiction).\n",
    "\n",
    "If we have time, it may also be worth distinguishing poetry reviewed in elite journals from poetry that wasn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>reception</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908</td>\n",
       "      <td>Robins, Elizabeth,</td>\n",
       "      <td>The convert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>looked like decent artisans, but more who bore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>Lytton, Edward Bulwer Lytton,</td>\n",
       "      <td>The coming race</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>called the \" Easy Time \" (with which what I ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1872</td>\n",
       "      <td>Butler, Samuel,</td>\n",
       "      <td>Erewhon, or, Over the range</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>the curtain ; on this I let it drop and retrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1900</td>\n",
       "      <td>Barrie, J. M.</td>\n",
       "      <td>Tommy and Grizel</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>at you !\" he said. \"Dear eyes, \" said she. \"Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873</td>\n",
       "      <td>Ritchie, Anne Thackeray,</td>\n",
       "      <td>Old Kensington</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>furious; I have not dared tell her, poor creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                         author                        title    genre  \\\n",
       "0  1908             Robins, Elizabeth,                  The convert  fiction   \n",
       "1  1871  Lytton, Edward Bulwer Lytton,              The coming race  fiction   \n",
       "2  1872                Butler, Samuel,  Erewhon, or, Over the range  fiction   \n",
       "3  1900                  Barrie, J. M.             Tommy and Grizel  fiction   \n",
       "4  1873       Ritchie, Anne Thackeray,               Old Kensington  fiction   \n",
       "\n",
       "  reception                                               text  \n",
       "0     elite  looked like decent artisans, but more who bore...  \n",
       "1     elite  called the \" Easy Time \" (with which what I ma...  \n",
       "2     elite  the curtain ; on this I let it drop and retrea...  \n",
       "3     elite  at you !\" he said. \"Dear eyes, \" said she. \"Th...  \n",
       "4     elite  furious; I have not dared tell her, poor creat...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relativepath = os.path.join('..', 'data', 'weekfour', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "poefic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Exercise 2\n",
    "\n",
    "# The main thing you will need to change is the code that\n",
    "# identifies rows as belonging to one of two corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using corpora to create a \"meaning space.\"\n",
    "\n",
    "Contrasting two corpora can be revealing, but sometimes we want to think about the relations between individual texts or words. To do that, we often represent them as vectors in a multi-dimensional space.\n",
    "\n",
    "The simplest way to do this is to create a DataFrame where rows are documents and columns are word — a document-term matrix. Here's a function that does that. It requires a pre-defined vocabulary (list of words) as well as a list (or numpy vector) of texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_term_matrix(vocab, textvector):\n",
    "    ''' Transform the textvector into a document-term matrix\n",
    "    with one column for each word in vocab.\n",
    "    '''\n",
    "    \n",
    "    n = len(textvector)\n",
    "    vocabset = set(vocab)\n",
    "    # making a set so we can check membership quickly;\n",
    "    # it's much faster in a set than in a list\n",
    "    \n",
    "    termdictionary = dict()\n",
    "    for word in vocab:\n",
    "        termdictionary[word] = np.zeros(n)\n",
    "    for i, text in enumerate(textvector):\n",
    "        counts = tokenize(text)\n",
    "        for word, count in counts.items():\n",
    "            if word in vocabset:\n",
    "                termdictionary[word][i] += count\n",
    "    \n",
    "    dtmatrix = pd.DataFrame(termdictionary, columns = vocab)\n",
    "    return dtmatrix\n",
    "\n",
    "# A nice arcane trick to perform on a document-term matrix\n",
    "# is to squash it into a smaller number of dimensions. This\n",
    "# often reveals relationships between words that don't\n",
    "# necessarily, literally occur together. The technique is called\n",
    "# Latent Semantic Analysis.\n",
    "\n",
    "def lsa_matrix(dtmatrix, vocab, number_of_dimensions):\n",
    "    lsa = TruncatedSVD(number_of_dimensions, algorithm = 'arpack')\n",
    "    dtm_lsa = lsa.fit_transform(dtmatrix)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "    lsamatrix = pd.DataFrame(lsa.components_, columns = vocab)\n",
    "    \n",
    "    return lsamatrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.sum(vector1 * vector2)\n",
    "    # we assume these are numpy vectors and can be\n",
    "    # multiplied elementwise\n",
    "    \n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Finding words that are close in \"meaning space.\"\n",
    "\n",
    "Following Widdows, we'll measure semantic similarity as the cosine similarity between vectors defined by a word's distribution across documents.\n",
    "\n",
    "Let's try this both in the space defined by Trump tweets and in the space defined by 19c literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>and</th>\n",
       "      <th>a</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>i</th>\n",
       "      <th>you</th>\n",
       "      <th>of</th>\n",
       "      <th>will</th>\n",
       "      <th>...</th>\n",
       "      <th>windham</th>\n",
       "      <th>nato's</th>\n",
       "      <th>9pm</th>\n",
       "      <th>@stephbewitching</th>\n",
       "      <th>supposedly</th>\n",
       "      <th>https://t.co/2towwtheef</th>\n",
       "      <th>https://t.co/n43cpejiqa</th>\n",
       "      <th>https://t.co/auszsnozlm</th>\n",
       "      <th>among</th>\n",
       "      <th>hitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   the   to  and    a   in   is    i  you   of  will   ...     windham  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0   ...         0.0   \n",
       "1  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   0.0   ...         0.0   \n",
       "2  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0   ...         0.0   \n",
       "3  1.0  0.0  1.0  2.0  0.0  1.0  0.0  0.0  1.0   0.0   ...         0.0   \n",
       "4  2.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0   ...         0.0   \n",
       "\n",
       "   nato's  9pm  @stephbewitching  supposedly  https://t.co/2towwtheef  \\\n",
       "0     0.0  0.0               0.0         0.0                      0.0   \n",
       "1     0.0  0.0               0.0         0.0                      0.0   \n",
       "2     0.0  0.0               0.0         0.0                      0.0   \n",
       "3     0.0  0.0               0.0         0.0                      0.0   \n",
       "4     0.0  0.0               0.0         0.0                      0.0   \n",
       "\n",
       "   https://t.co/n43cpejiqa  https://t.co/auszsnozlm  among  hitting  \n",
       "0                      0.0                      0.0    0.0      0.0  \n",
       "1                      0.0                      0.0    0.0      0.0  \n",
       "2                      0.0                      0.0    0.0      0.0  \n",
       "3                      0.0                      0.0    0.0      0.0  \n",
       "4                      0.0                      0.0    0.0      0.0  \n",
       "\n",
       "[5 rows x 4682 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = create_vocab(trump['text'], 5000)\n",
    "dtm = doc_term_matrix(vocab, trump['text'])\n",
    "lsa = lsa_matrix(dtm, vocab, 25)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for exercise 3\n",
    "\n",
    "# Let's start by getting a vocabulary,\n",
    "# and a doc-term matrix, as well as a\n",
    "# squashed (lsa) version of that matrix.\n",
    "\n",
    "vocab = create_vocab(trump['text'], 3000)\n",
    "dtm = doc_term_matrix(vocab, trump['text'])\n",
    "lsa = lsa_matrix(dtm, vocab, 25)\n",
    "\n",
    "def find_matches(amatrix, word):\n",
    "    columnA = amatrix[word]\n",
    "    \n",
    "    vocab = amatrix.columns.values\n",
    "    tuplelist = []\n",
    "    for w in vocab:\n",
    "        columnB = amatrix[w]\n",
    "        cosinesim = cosine_similarity(columnA, columnB)\n",
    "        tuplelist.append((cosinesim, w))\n",
    "    \n",
    "    headandtail(tuplelist, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word? weak\n",
      "TOP VALUES:\n",
      "weak 1.0\n",
      "openly 0.6872722879003443\n",
      "losses 0.626152250163958\n",
      "desperate 0.6224385022881581\n",
      "team 0.6223334214785835\n",
      "leadership 0.6176981361352789\n",
      "act 0.6161027847845036\n",
      "unfit 0.5817734047796101\n",
      "https://t.co/3ezg620fpt 0.5691954802090683\n",
      "figure,wants 0.5563717702230214\n",
      "\n",
      "BOTTOM VALUES:\n",
      "ryan -0.5659272121712704\n",
      "picks -0.5111880328998333\n",
      "donât -0.49501014875028715\n",
      "meaningless -0.46681146235839505\n",
      "went -0.454161210091449\n",
      "hope -0.45236006444886045\n",
      "front -0.43537197005938383\n",
      "page -0.43537197005938383\n",
      "megan -0.4325629283517442\n",
      "$1 -0.43243588978981606\n"
     ]
    }
   ],
   "source": [
    "user_word = input('word? ')\n",
    "find_matches(lsa, user_word) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
