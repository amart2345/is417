{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranking-literacy",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes\n",
    "\n",
    "We're going to walk through an implementation of a Naive Bayes classifier, and in doing so get comfortable with some basic aspects of machine learning on collections of text.\n",
    "\n",
    "In the first week of this course we explored basic input and output using Python, and also reviewed some string operations that we can use to \"tokenize\" text (divide it into words or word-like tokens) and \"normalize\" it (for instance, by rendering everything lowercase). We could continue using those basic Python functions to convert the texts we use into numbers.\n",
    "\n",
    "But since normalizing and tokenizing text is a very common operation, there are Python libraries that take care of it for us. Using them will simplify our code. Standard libraries also reduces sources of distortion that can creep into a model when (say) the model trained using one tokenizing process, and then applied to data produced with a different process.\n",
    "\n",
    "### Using CountVectorizer to turn texts into a pandas dataframe\n",
    "\n",
    "One library we'll use a lot is [scikit-learn.](https://scikit-learn.org/stable/about.html) As a package it's abbreviated ```sklearn.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "electronic-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn   # only uncomment and run if needed\n",
    "                        # in other words, if you get an error when you attempt\n",
    "                        # to import sklearn below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "raised-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "unavailable-enclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>boxing</th>\n",
       "      <th>film</th>\n",
       "      <th>great</th>\n",
       "      <th>it</th>\n",
       "      <th>no</th>\n",
       "      <th>or</th>\n",
       "      <th>part</th>\n",
       "      <th>pathetic</th>\n",
       "      <th>plot</th>\n",
       "      <th>satire</th>\n",
       "      <th>scenes</th>\n",
       "      <th>the</th>\n",
       "      <th>twists</th>\n",
       "      <th>was</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reviewA</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewC</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         and  boxing  film  great  it  no  or  part  pathetic  plot  satire  \\\n",
       "reviewA    0       1     0      0   1   0   0     1         1     0       0   \n",
       "reviewB    0       0     0      1   0   1   1     0         0     1       0   \n",
       "reviewC    2       0     0      1   0   0   0     0         0     1       1   \n",
       "reviewD    0       0     1      2   0   0   0     0         0     0       0   \n",
       "\n",
       "         scenes  the  twists  was  worst  \n",
       "reviewA       1    2       0    2      1  \n",
       "reviewB       1    0       1    0      0  \n",
       "reviewC       0    0       1    0      0  \n",
       "reviewD       1    0       0    0      0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = [\"It was pathetic. The worst part was the boxing scenes.\",\n",
    "         \"No plot twists or great scenes.\",\n",
    "         \"and satire, and great plot twists\",\n",
    "         \"Great scenes; great film.\"]\n",
    "text_titles = ['reviewA', 'reviewB', 'reviewC', 'reviewD']\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectors = count_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "vector_frame = pd.DataFrame(count_vectors.toarray(), index = text_titles, \n",
    "                            columns = count_vectorizer.get_feature_names())\n",
    "vector_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "frozen-dietary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and         2\n",
       "boxing      1\n",
       "film        1\n",
       "great       4\n",
       "it          1\n",
       "no          1\n",
       "or          1\n",
       "part        1\n",
       "pathetic    1\n",
       "plot        2\n",
       "satire      1\n",
       "scenes      3\n",
       "the         2\n",
       "twists      2\n",
       "was         2\n",
       "worst       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_frame.sum(axis = 'rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-attraction",
   "metadata": {},
   "source": [
    "We'll call this a term-doc matrix; typically words (\"features\") are columns and documents are rows.\n",
    "\n",
    "A \"vector\" is essentially a list of numbers. Next week we'll talk about the geometrical interpretation that makes it possible to interpret a list of numbers as a line in space.\n",
    "\n",
    "### Relative frequencies (normalized by doc length)\n",
    "\n",
    "Suppose we wanted to have the relative frequency of each word as a percentage of its document. That representation has a lot of advantages, since it factors out document length and provides, essentially, a unigram probability model.\n",
    "\n",
    "Before proceeding, pause for a moment and think about what we would need to do mathematically to generate relative frequencies. Then you'll understand the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "internal-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewA    10\n",
       "reviewB     6\n",
       "reviewC     6\n",
       "reviewD     4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowsums = vector_frame.sum(axis = 'columns')   # change to 'rows' and see what happens\n",
    "rowsums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-coordination",
   "metadata": {},
   "source": [
    "For future reference, ```axis = 'columns'``` is often abbreviated ```axis = 1``` and ```axis = 'rows'``` is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "spoken-spanish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>boxing</th>\n",
       "      <th>film</th>\n",
       "      <th>great</th>\n",
       "      <th>it</th>\n",
       "      <th>no</th>\n",
       "      <th>or</th>\n",
       "      <th>part</th>\n",
       "      <th>pathetic</th>\n",
       "      <th>plot</th>\n",
       "      <th>satire</th>\n",
       "      <th>scenes</th>\n",
       "      <th>the</th>\n",
       "      <th>twists</th>\n",
       "      <th>was</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reviewA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewB</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewC</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              and  boxing  film     great   it        no        or  part  \\\n",
       "reviewA  0.000000     0.1  0.00  0.000000  0.1  0.000000  0.000000   0.1   \n",
       "reviewB  0.000000     0.0  0.00  0.166667  0.0  0.166667  0.166667   0.0   \n",
       "reviewC  0.333333     0.0  0.00  0.166667  0.0  0.000000  0.000000   0.0   \n",
       "reviewD  0.000000     0.0  0.25  0.500000  0.0  0.000000  0.000000   0.0   \n",
       "\n",
       "         pathetic      plot    satire    scenes  the    twists  was  worst  \n",
       "reviewA       0.1  0.000000  0.000000  0.100000  0.2  0.000000  0.2    0.1  \n",
       "reviewB       0.0  0.166667  0.000000  0.166667  0.0  0.166667  0.0    0.0  \n",
       "reviewC       0.0  0.166667  0.166667  0.000000  0.0  0.166667  0.0    0.0  \n",
       "reviewD       0.0  0.000000  0.000000  0.250000  0.0  0.000000  0.0    0.0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_frame.divide(rowsums, axis = 'rows')  # change to 'columns' and see what happens :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-capability",
   "metadata": {},
   "source": [
    "### Load folders of files into a dataframe\n",
    "\n",
    "We're using [a dataset of movie reviews developed by Bo Pang, Lillian Lee,and Shivakumar Vaidhyanathan.](https://www.cs.cornell.edu/people/pabo/movie-review-data/)\n",
    "\n",
    "Our first task is to load them into a dataframe.\n",
    "\n",
    "It's worth stepping through this to make sure you understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "canadian-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dir = '../../data/review_polarity/txt_sentoken/neg'\n",
    "positive_dir = '../../data/review_polarity/txt_sentoken/pos'\n",
    "neg_paths = glob.glob(f'{negative_dir}/*.txt')\n",
    "pos_paths = glob.glob(f'{positive_dir}/*.txt')\n",
    "\n",
    "all_paths = neg_paths + pos_paths      # notice the order\n",
    "\n",
    "all_classes = [0] * 1000 + [1] * 1000  # notice the same order\n",
    "                                       # if it's not clear what's in that list, inspect\n",
    "                                       # by using len() and saying all_classes[-10 : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-identity",
   "metadata": {},
   "source": [
    "We now have a list of 2000 paths to files, paired with a list of class labels that are either zero (negative sentiment) or one (positive sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fabulous-seafood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>zane</th>\n",
       "      <th>zany</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zone</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv676_22202</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv839_22807</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv155_7845</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv465_23401</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv398_17047</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             000  10  100  11  12  13  13th  14  15  16  ...  younger  your  \\\n",
       "cv676_22202    0   0    0   0   0   0     0   0   0   0  ...        0     1   \n",
       "cv839_22807    0   0    0   0   0   0     0   0   0   0  ...        1     0   \n",
       "cv155_7845     0   0    0   0   0   1     0   0   0   0  ...        0     0   \n",
       "cv465_23401    0   0    0   0   0   0     0   0   0   0  ...        0     0   \n",
       "cv398_17047    0   0    0   0   0   0     0   0   0   0  ...        0     0   \n",
       "\n",
       "             yourself  youth  zane  zany  zero  zeta  zone  class_label  \n",
       "cv676_22202         1      0     0     0     0     0     0            0  \n",
       "cv839_22807         0      0     0     0     0     0     0            0  \n",
       "cv155_7845          0      0     0     0     0     0     0            0  \n",
       "cv465_23401         0      0     0     0     0     0     0            0  \n",
       "cv398_17047         0      0     0     0     0     0     0            0  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(input = 'filename',   # notice that we're now setting this up\n",
    "                                  max_features = 5000)   # to automatically read a list of paths\n",
    "                                                       # and also only taking the top 5000 words\n",
    "    \n",
    "word_counts = count_vectorizer.fit_transform(all_paths)  # that line does all the work!\n",
    "\n",
    "titles = [Path(text).stem for text in all_paths]\n",
    "count_df = pd.DataFrame(word_counts.toarray(), index = titles, \n",
    "                      columns = count_vectorizer.get_feature_names())\n",
    "\n",
    "count_df = count_df.assign(class_label = all_classes)  # adding a column for class_label\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "biological-accent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-wallet",
   "metadata": {},
   "source": [
    "We now have a term-doc matrix for the top 5000 words in 2000 movie reviews, along with a column that indicates whether each review was negative or positive.\n",
    "\n",
    "### Generating train and test sets\n",
    "\n",
    "Now let's divide this into train and test sets. By convention we're going to call the matrix of feature values *X* (it gets a capital because the names of matrices are by convention capital letters). The vector of class labels, we'll call *y.* We're about to learn a function that predicts *y* from *X.* To be super-fancy we can refer to our predictions as $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "complex-dispute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 5000)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_counts = count_df.loc[count_df['class_label'] == 0, ]\n",
    "pos_counts = count_df.loc[count_df['class_label'] == 1, ]\n",
    "\n",
    "train_X = pd.concat([neg_counts.iloc[0:800, : ], pos_counts.iloc[0:800, : ]], axis = 'rows')\n",
    "train_y = train_X['class_label']\n",
    "train_X = train_X.drop('class_label', axis = 'columns')  # we don't want this as a feature. Why not?\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-lithuania",
   "metadata": {},
   "source": [
    "Notice that we just took the first 800 elements of the negative and positive dataframes as our training set. (That's four-fifths of the data). We're trusting that the data is already well-randomized, so all the reviews of action movies aren't at the end, etc.\n",
    "\n",
    "#### a quick thought experiment\n",
    "\n",
    "I wrote a line of code that drops the class_label from train_X.\n",
    "\n",
    "    train_X = train_X.drop('class_label', axis = 'columns')\n",
    "    \n",
    "What would happen if I forgot to do that, and we trained a naive Bayes model to predict $\\hat{y}$ on train_X with that extra column? What would you expect our accuracy to be?\n",
    "\n",
    "Now write some code that generates a test set. We'll need both a matrix of feature values and a vector of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "alternative-senate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5000)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lines that create a test_X and a test_y\n",
    "\n",
    "test_X = pd.concat([neg_counts.iloc[800: , : ], pos_counts.iloc[800: , : ]], axis = 'rows')\n",
    "test_y = test_X['class_label']\n",
    "test_X = test_X.drop('class_label', axis = 'columns')\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-jacksonville",
   "metadata": {},
   "source": [
    "At the end your shape should be (400, 5000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-workshop",
   "metadata": {},
   "source": [
    "### Applying Naive Bayes can be simple, if we just want results\n",
    "\n",
    "If our goal is simply to get predictions, that's easy. Scikit-learn has [several forms of Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) built in. We'll use [Multinomial Naive Bayes.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "informal-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB(alpha = 1)\n",
    "bayes.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "median-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = bayes.predict(test_X)\n",
    "yhat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "documentary-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(yhat == test_y) / len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-petersburg",
   "metadata": {},
   "source": [
    "### But let's implement naive Bayes ourselves, to understand it. First, training.\n",
    "\n",
    "Let's review the pseudo-code from Jurafsky and Martin:\n",
    "\n",
    "![pseudocode for the train function](pseudocode_train.png)\n",
    "\n",
    "Because the probabilities we're dealing with are very, very small, and get smaller as you multiply them, naive Bayes is conventionally implemented by adding logarithms instead of multiplying probabilities. To confirm that this is (or should be!) the same thing, check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "female-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010000000000000002\n",
      "0.010000000000000004\n"
     ]
    }
   ],
   "source": [
    "logsum = math.log(0.1) + math.log(0.1)\n",
    "\n",
    "product = 0.1 * 0.1\n",
    "\n",
    "print(product)\n",
    "print(math.exp(logsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-newsletter",
   "metadata": {},
   "source": [
    "Floating-point math isn't perfect! but that's basically the same thing.\n",
    "\n",
    "Okay, let's generate the class priors, expressed as logarithms.\n",
    "\n",
    "The code for this is actually extremely simple. *Notice that we do not have to construct a for-loop at all, because pandas takes care of it for us!* We just count the number of instances in each class, and divide by the total number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "equipped-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(X, y):\n",
    "    '''\n",
    "    This function only performs the first part of the training:\n",
    "    it generates a class prior for each class, which takes the form\n",
    "    of a pandas Series\n",
    "    '''\n",
    "    \n",
    "    priors = y.value_counts() / y.shape[0]\n",
    "    \n",
    "    logpriors = np.log(priors)\n",
    "    \n",
    "    return logpriors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "played-grenada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: class_label, dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts() / 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "magnetic-publication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function we just defined to generate a class prior, and print\n",
    "# it out to see what it looks like.\n",
    "\n",
    "# Then play around with the math here, step by step,\n",
    "# to understand why that works. Test .value_counts() and shape,\n",
    "# and then divide, etc\n",
    "\n",
    "# To confirm that we're getting the right value, look at this:\n",
    "\n",
    "math.log(800/ 1600)\n",
    "\n",
    "# the difference between math.log() and np.log() is that\n",
    "# the numpy version automatically *broadcasts* the function\n",
    "# across a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-england",
   "metadata": {},
   "source": [
    "The next part is almost equally simple. Again, no for-loop! Remember how we summed up the rows to normalize frequencies? Now we can sum columns to find the total number of times a word appears in a class. Pandas does that on the whole dataframe at once and saves us from writing a loop, which would tend to be slower as well as more verbose. What we need to do is, step by step.\n",
    "\n",
    "```For each class c:```\n",
    "\n",
    "    1) Create a dataframe by selecting rows with class == c. We can use train_y to do that. This creates what Jurafsky and Martin call ```bigdoc```. Then, for each class:\n",
    "\n",
    "    2) Sum the *columns* of that class frame to create a vector of wordcounts for the class. Now we have ```count(w, c)```. We'll call that a classvector.\n",
    "\n",
    "    3) Add one to all the elements of the classvector (Laplace smoothing).\n",
    "    \n",
    "    4) Divide the smoothed classvector by its own sum, producing a vector of smoothed likelihoods.\n",
    "\n",
    "    5) Take the np.log() of the smoothed likelihoods.\n",
    "\n",
    "    6) Package the two vectors in a single data frame or dictionary. Voila! You have your loglikelihoods.\n",
    "   \n",
    "Let's work through that step by step. First, turn train_X into two classvectors. There are ways to do even the two classes without a loop, but let's not get too fancy with it right now; it's okay to loop across the classes and store the resulting vectors in a dictionary.\n",
    "\n",
    "This takes us up to step three, which is a good place to pause and inspect the results to see what we've actually got.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "adverse-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKSPACE for TRAINING\n",
    "\n",
    "counts = dict()\n",
    "\n",
    "for c in [0, 1]:        # instead of [0,1] you can say np.unique(train_y) if you prefer\n",
    "    \n",
    "    class_frame = train_X.loc[train_y == c, : ]\n",
    "    colsums = class_frame.sum(axis = 'rows')\n",
    "    counts[c] = colsums\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-prospect",
   "metadata": {},
   "source": [
    "At this point we have raw counts of the number of words in positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "narrative-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000      54\n",
       "10      159\n",
       "100      33\n",
       "11       15\n",
       "12       22\n",
       "       ... \n",
       "zane      4\n",
       "zany     13\n",
       "zero     32\n",
       "zeta     24\n",
       "zone     16\n",
       "Length: 5000, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0] # negative movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-pricing",
   "metadata": {},
   "source": [
    "We can smooth those vectors by adding one to them, so even rare words will have some chance of inclusion. Then, if we divide by the total number of words, we'll have probabilities we can call \"likelihoods\" (the probability of an outcome, given a set of parameter values, can also be viewed the other way around, as the likelihood of the parameter values if we've observed the outcome). Here the \"parameter values\" are words and the class label is the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "universal-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = dict()\n",
    "\n",
    "for c in [0, 1]:\n",
    "    counts[c] = counts[c] + 1\n",
    "    likelihoods[c] = counts[c] / np.sum(counts[c])\n",
    "    \n",
    "ratios = likelihoods[0] / likelihoods[1]\n",
    "ratios.sort_values(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "surrounded-fluid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shrek        0.017515\n",
       "ordell       0.023851\n",
       "gattaca      0.027341\n",
       "mulan        0.029116\n",
       "ripley       0.035031\n",
       "leila        0.035031\n",
       "sweetback    0.036161\n",
       "mallory      0.038654\n",
       "lambeau      0.038654\n",
       "gladiator    0.040035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "genuine-photograph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jawbreaker    15.693681\n",
       "jakob         15.693681\n",
       "magoo         16.254170\n",
       "webb          17.375147\n",
       "memphis       20.177590\n",
       "palmetto      23.540522\n",
       "bats          23.540522\n",
       "sphere        28.024431\n",
       "brenner       35.871271\n",
       "jolie         41.476157\n",
       "dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios[-10 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "governmental-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8668890556884498"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios['exciting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "residential-might",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.757317988534175"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios['boring']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-anchor",
   "metadata": {},
   "source": [
    "## The finished train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eligible-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(X, y):\n",
    "    '''\n",
    "    This function does the\n",
    "    '''\n",
    "    \n",
    "    priors = y.value_counts() / y.shape[0]\n",
    "    \n",
    "    logpriors = np.log(priors)\n",
    "    \n",
    "    loglikelihoods = dict()\n",
    "    \n",
    "    for c in np.unique(y):\n",
    "    \n",
    "        class_frame = train_X.loc[train_y == c, : ]\n",
    "        colsums = class_frame.sum(axis = 'rows') + 1\n",
    "        likelihoods = colsums / np.sum(colsums)\n",
    "        loglikelihoods[c] = np.log(likelihoods)   \n",
    "    \n",
    "    return logpriors, loglikelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "hydraulic-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpriors, loglikelihoods = train_bayes(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "filled-terror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blatantly     -11.134372\n",
       "bleak          -9.712986\n",
       "blend         -10.441225\n",
       "blind          -9.861406\n",
       "block          -9.989240\n",
       "blockbuster   -10.135843\n",
       "blond         -10.777697\n",
       "blonde        -10.307694\n",
       "blood          -8.649465\n",
       "bloody         -9.583775\n",
       "dtype: float64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect to see what you've got:\n",
    "\n",
    "loglikelihoods[1][495:505]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-messenger",
   "metadata": {},
   "source": [
    "## Now write a function that uses our Naive Bayes model to make predictions about test_X\n",
    "\n",
    "Here's a reminder of the function we need:\n",
    "\n",
    "![pseudocode for the test function](pseudocode_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-basic",
   "metadata": {},
   "source": [
    "This becomes quite simple when you realize that there's no need to do a loop across \"positions.\" We've already counted words, and adding the loglikelihood[word] to the probability each time you see a word is the same as adding ```loglikelihood[word] * occurences_of_word.```  \n",
    "\n",
    "Well each row in ```test_X``` has the number of occurrences of the word. So we need to multiply each entry in that row by the corresponding entry in the loglikelihood vector.  In pandas, the multiplication symbol automatically does elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "incoming-transmission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    0\n",
       "b    1\n",
       "c    2\n",
       "d    3\n",
       "e    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_example = pd.Series([0,1,2,3,4], index = ['a', 'b', 'c', 'd', 'e'])\n",
    "vector_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "floral-married",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    0\n",
       "b    2\n",
       "c    4\n",
       "d    6\n",
       "e    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_example * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "precise-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     0\n",
       "b     1\n",
       "c     4\n",
       "d     9\n",
       "e    16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_example * vector_example    # note elementwise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "recreational-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bayes(logpriors, loglikelihoods, test_X):\n",
    "    '''\n",
    "    Tests a model that is passed in as \"logpriors\" and\n",
    "    \"loglikelihoods\" against the data passed in as test_X,\n",
    "    and returns a vector of predictions.\n",
    "    '''\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    for idx in test_X.index:\n",
    "        class_probabilities = dict()\n",
    "        for c in [0, 1]:\n",
    "            probability_sum = logpriors[c]\n",
    "            this_row = test_X.loc[idx, : ]\n",
    "            probability_sum = probability_sum + np.sum(loglikelihoods[c] * this_row) \n",
    "            class_probabilities[c] = probability_sum\n",
    "            \n",
    "        if class_probabilities[1] > class_probabilities[0]:   # this is the argmax part\n",
    "            predictions.append(1)                          # we choose the class with the larger probability\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "invalid-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_bayes(logpriors, loglikelihoods, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "imported-content",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions == test_y) / len(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
